<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Case-1 Local LLM Test (Qwen2 0.5B)</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />

  <style>
    body {
      font-family: system-ui, sans-serif;
      background: #0f172a;
      color: #e5e7eb;
      padding: 20px;
    }
    button {
      padding: 10px 16px;
      font-size: 16px;
      cursor: pointer;
      margin-top: 10px;
    }
    pre {
      background: #020617;
      padding: 12px;
      border-radius: 6px;
      overflow-x: auto;
      max-height: 400px;
    }
    .ok { color: #22c55e; }
    .err { color: #ef4444; }
    .info { color: #38bdf8; }
  </style>
</head>

<body>
  <h2>üß™ Case-1: Fully Local Browser LLM</h2>
  <p>
    Model: <b>Qwen2-0.5B-Instruct (CPU / WASM)</b><br>
    Environment: Browser-only (no backend)
  </p>

  <button id="loadBtn">Load Model</button>
  <button id="runBtn" disabled>Run Test Prompt</button>

  <pre id="log"></pre>

<script type="module">
/* ============================================================
   IMPORTANT: MUST BE SERVED OVER HTTPS (GitHub Pages)
============================================================ */

import {
  pipeline,
  env
} from "https://cdn.jsdelivr.net/npm/@xenova/transformers@2.17.2";

/* ---------- FORCE SAFE EXECUTION ---------- */
env.allowLocalModels = false;
env.useBrowserCache = true;
env.backends.webgpu.enabled = false; // ‚ùå GPU OFF
env.backends.wasm.enabled = true;     // ‚úÖ CPU WASM
env.backends.onnx.wasm.numThreads = 1;

/* ---------- UI ---------- */
const logBox = document.getElementById("log");
const loadBtn = document.getElementById("loadBtn");
const runBtn = document.getElementById("runBtn");

function log(msg, cls = "info") {
  logBox.innerHTML += `<div class="${cls}">${msg}</div>`;
  logBox.scrollTop = logBox.scrollHeight;
}

/* ---------- LLM ---------- */
let generator = null;

loadBtn.onclick = async () => {
  try {
    log("üîÑ Loading model‚Ä¶ (first load = slow)", "info");

    generator = await pipeline(
      "text-generation",
      "Qwen/Qwen2-0.5B-Instruct",
      {
        device: "cpu",
        dtype: "q8"
      }
    );

    log("‚úÖ Model loaded successfully!", "ok");
    runBtn.disabled = false;

  } catch (err) {
    console.error(err);
    log("‚ùå Model load failed", "err");
    log(err.message || err.toString(), "err");
  }
};

runBtn.onclick = async () => {
  try {
    log("‚ñ∂Ô∏è Running inference‚Ä¶", "info");

    const out = await generator(
      "User: Order two veg pizzas and one coke\nAssistant:",
      {
        max_new_tokens: 60,
        temperature: 0.3
      }
    );

    log("üß† LLM Output:", "ok");
    log(out[0].generated_text, "info");

  } catch (err) {
    console.error(err);
    log("‚ùå Inference failed", "err");
    log(err.message || err.toString(), "err");
  }
};
</script>
</body>
</html>
