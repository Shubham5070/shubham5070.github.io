<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>T1: Fully Local LLM Voice Test (WASM Fix)</title>
    <script src="https://cdn.jsdelivr.net/npm/@xenova/transformers@2.14.0"></script>
    <style>
        body { font-family: sans-serif; text-align: center; padding: 20px; background-color: #f0f8ff; }
        button {
            padding: 15px 30px; font-size: 18px; background-color: #3498db; color: white; border: none;
            border-radius: 50px; cursor: pointer; margin-top: 20px; box-shadow: 0 4px 10px rgba(0, 0, 0, 0.1);
        }
        #output { margin-top: 30px; padding: 15px; border: 1px solid #ccc; background-color: white; border-radius: 8px; min-height: 80px; text-align: left; }
        .status { font-weight: bold; margin-top: 15px; color: #e74c3c; }
        .user-text { color: #2980b9; }
        .bot-text { color: #27ae60; }
    </style>
</head>
<body>

    <h1>üß† Local LLM Test (WASM/CPU)</h1>
    <div class="status" id="status">Loading model... Please wait.</div>
    <button id="listenButton" onclick="startListening()" disabled>Tap to Speak Order</button>
    <div id="output"></div>

    <script>
        // --- CONFIGURATION ---
        // Using a highly stable model name that defaults to ONNX/WASM for better compatibility
        const MODEL_ID = 'Xenova/Qwen2-0.5B-Instruct'; 
        const MAX_NEW_TOKENS = 50;

        // --- GLOBAL VARIABLES & INITIAL LOAD ---
        let generator = null;
        const statusDiv = document.getElementById('status');
        const listenButton = document.getElementById('listenButton');

        async function loadLLM() {
            try {
                statusDiv.textContent = `Downloading and loading ${MODEL_ID} using WASM... (This may take a minute.)`;
                
                // Set environment to maximize stability
                // This explicitly requests the CPU/WASM backend and forces quantization (q8)
                transformers.env.backends.onnx.wasm.proxy = false;

                generator = await transformers.pipeline(
                    "text-generation", 
                    MODEL_ID, 
                    { 
                        device: "cpu", 
                        quantized: true, 
                        dtype: "q8",
                        // Pass specific environment settings during pipeline creation
                        env: transformers.env
                    } 
                );

                statusDiv.textContent = "‚úÖ Model loaded successfully. Click to speak.";
                listenButton.disabled = false;
            } catch (error) {
                statusDiv.textContent = "‚ùå ERROR: Model failed to load. Check console for advanced error details. This setup may not be possible on your device.";
                console.error("LLM Load Error:", error);
            }
        }

        // Start model loading immediately
        loadLLM(); 

        // --- STT and TTS Logic (REMAINS THE SAME) ---
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;

        if (SpeechRecognition) {
            const recognition = new SpeechRecognition();
            recognition.lang = 'en-US';
            recognition.continuous = false;
            recognition.interimResults = false;

            recognition.onstart = () => {
                statusDiv.textContent = "Listening... Speak now.";
                listenButton.disabled = true;
            };

            recognition.onresult = (event) => {
                const transcript = event.results[0][0].transcript;
                output.innerHTML = `<span class="user-text">You said:</span> ${transcript}<br>`;
                statusDiv.textContent = "Processing order locally...";
                processOrder(transcript);
            };

            recognition.onerror = (event) => {
                statusDiv.textContent = `Error: ${event.error}. Click to try again.`;
                listenButton.disabled = false;
                window.speechSynthesis.cancel(); 
            };

            recognition.onend = () => {
                if (statusDiv.textContent === "Listening... Speak now.") {
                     statusDiv.textContent = "No speech detected. Click to try again.";
                     listenButton.disabled = false;
                }
            };
            
            window.startListening = () => {
                if (generator) {
                    window.speechSynthesis.cancel(); 
                    recognition.start();
                } else {
                    statusDiv.textContent = "Model is not ready yet. Please wait.";
                }
            };
        } else {
            statusDiv.textContent = "‚ùå Speech Recognition not supported.";
        }
        
        async function processOrder(transcript) {
            try {
                const messages = [
                    {"role": "system", "content": "You are a fast, local restaurant menu chatbot. Keep your responses short and focus on taking an order."},
                    {"role": "user", "content": transcript}
                ];

                const outputs = await generator(
                    messages,
                    { max_new_tokens: MAX_NEW_TOKENS }
                );

                const llmResponse = outputs[0].generated_text.trim();
                output.innerHTML += `<span class="bot-text">Bot response:</span> ${llmResponse}`;
                speakResponse(llmResponse);
                
            } catch (error) {
                const errorMessage = "Local processing error. Try reloading.";
                output.innerHTML += `<span class="bot-text">Bot response:</span> ${errorMessage}`;
                speakResponse(errorMessage);
                console.error("Local LLM Error:", error);
            }
        }

        function speakResponse(text) {
            const utterance = new SpeechSynthesisUtterance(text);
            utterance.rate = 1.05; 
            
            utterance.onstart = () => { statusDiv.textContent = "Bot is speaking..."; };
            utterance.onend = () => { 
                statusDiv.textContent = "Conversation complete. Click to start a new order.";
                listenButton.disabled = false;
            };
            window.speechSynthesis.speak(utterance);
        }
    </script>
</body>
</html>
